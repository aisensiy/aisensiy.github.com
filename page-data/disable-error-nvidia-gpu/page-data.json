{
    "componentChunkName": "component---src-templates-blog-js",
    "path": "/disable-error-nvidia-gpu",
    "result": {"data":{"blog":{"id":"0d324ce7-77fc-530b-b2ea-97bdb60e8b51","html":"<p>又发现了集群里出现了挂掉的 nvidia gpu 这里记录下如何屏蔽掉它以保证其他 GPU 可以继续被使用。</p>\n<p>首先是监控告警，告知 <code>nvidia-smi</code> 命令出错了，去机器上看一下有这么个错误：</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ nvidia-smi\nUnable to determine the device handle <span class=\"token keyword\">for</span> GPU 0000:89:00.0: Unknown Error</code></pre></div>\n<p>感觉是这块卡 <code>0000:89:00.0</code> 出问题了。然后去执行下 <code>dmesg</code> 看看情况：</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">dmesg</span> -T\n<span class=\"token punctuation\">[</span>Mon May  <span class=\"token number\">9</span> <span class=\"token number\">20</span>:37:33 <span class=\"token number\">2022</span><span class=\"token punctuation\">]</span> xhci_hcd 0000:89:00.2: PCI post-resume error -19<span class=\"token operator\">!</span>\n<span class=\"token punctuation\">[</span>Mon May  <span class=\"token number\">9</span> <span class=\"token number\">20</span>:37:33 <span class=\"token number\">2022</span><span class=\"token punctuation\">]</span> xhci_hcd 0000:89:00.2: HC died<span class=\"token punctuation\">;</span> cleaning up\n<span class=\"token punctuation\">[</span>Mon May  <span class=\"token number\">9</span> <span class=\"token number\">20</span>:37:34 <span class=\"token number\">2022</span><span class=\"token punctuation\">]</span> nvidia-gpu 0000:89:00.3: i2c <span class=\"token function\">timeout</span> error ffffffff\n<span class=\"token punctuation\">[</span>Mon May  <span class=\"token number\">9</span> <span class=\"token number\">20</span>:37:34 <span class=\"token number\">2022</span><span class=\"token punctuation\">]</span> ucsi_ccg <span class=\"token number\">6</span>-0008: i2c_transfer failed -110</code></pre></div>\n<p>看不懂，搜了搜也有点懵逼。这台机器已经运行了挺久了，驱动也在短期内没有出过问题，那么就感觉是硬件出问题了。重启机器后，<code>nvidia-smi</code> 恢复了。不过如果有其他任务使用了 GPU 就又会出现这个问题了。所以考虑使用 <code>nvidia-smi</code> 的命令先屏蔽掉这块报错的 GPU 。</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ nvidia-smi drain  -p 0000:89:00.0 -m <span class=\"token number\">1</span>\nSuccessfully <span class=\"token builtin class-name\">set</span> GPU 00000000:89:00.0 drain state to: draining.</code></pre></div>\n<p>然后再执行命令 <code>nvidia-smi</code> 就看不到这块卡了。</p>","tableOfContents":"","frontmatter":{"title":"记录 nvidia gpu 报错处理","date":"May 10, 2022","tags":["gpu","nvidia"]},"excerpt":"又发现了集群里出现了挂掉的 nvidia gpu 这里记录下如何屏蔽掉它以保证其他 GPU 可以继…"}},"pageContext":{"id":"0d324ce7-77fc-530b-b2ea-97bdb60e8b51"}},
    "staticQueryHashes": ["4202924991"]}