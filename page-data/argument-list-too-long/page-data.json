{"componentChunkName":"component---src-templates-blog-js","path":"/argument-list-too-long/","result":{"data":{"blog":{"id":"dd5ea23c-07d0-52ca-800e-752af7089f06","html":"<h2 id=\"事故原因\" style=\"position:relative;\">事故原因<a href=\"#%E4%BA%8B%E6%95%85%E5%8E%9F%E5%9B%A0\" aria-label=\"事故原因 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h2>\n<p>前天晚上 k8s 集群出现了问题，一开始是有一台 master 的内存不够了，然后导致了一些 k8s 系统的 pod 跪了，在对 master 节点升级之后开始去重启那些受影响的 crd controller，结果发现这个 crd controller 报错：</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/880fc4928a6a970ad5192932030a7cec/7161f/k8s-argument-list-too-long.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 13.999999999999998%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAmklEQVQI1zWOaw6CMBCEOYwaMSKPSqFPbGkhJGq8/2XG7BJ+TLrT2Z18hXYZg57R9o7f5/iC9gtGm9BJjyPvVYDUkaVs4rwf979aWJzLDpebQBHSGy5s8GFDzB/My3f3cfd2WtlTsZlW5O0H4xcol7lUyIkB7o3C6dqiqFqNWhgmo5BIaKaFQUc+enSG1QjLNJSXleQCIjtEhH8TBmRV0AWZCgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"k8s argument list too long\"\n        title=\"k8s argument list too long\"\n        src=\"/static/880fc4928a6a970ad5192932030a7cec/5a190/k8s-argument-list-too-long.png\"\n        srcset=\"/static/880fc4928a6a970ad5192932030a7cec/772e8/k8s-argument-list-too-long.png 200w,\n/static/880fc4928a6a970ad5192932030a7cec/e17e5/k8s-argument-list-too-long.png 400w,\n/static/880fc4928a6a970ad5192932030a7cec/5a190/k8s-argument-list-too-long.png 800w,\n/static/880fc4928a6a970ad5192932030a7cec/7161f/k8s-argument-list-too-long.png 1182w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>看到这个报错我很懵逼：我代码都没改，怎么突然就会报错呢，我启动的 <code>argument list</code> 也不长。连续重启两次发现没有什么效果就又去尝试重启其他的 pod 然后发现其他的 pod 也会报同样的错误...导致了问题的扩大。而且这个 crd controller 还是很重要的组件，它启动不起来由它管理的 crd 就都不能更新了，非常头大。</p>\n<p>然后就去网上查资料，发现了一个几乎和我们集群一模一样的问题的帖子：<a href=\"https://devpress.csdn.net/k8s/62ee60517e6682346618232e.html\">事后分析:Kubernetes Pod 无法启动,因为服务太多</a> ，总结来说就是因为 k8s 默认会把一个 namespace 的 service 创建一组环境变量，然后把这些环境变量注入到 namespace 所有的 pod 里。service 越多，在 pod 里注入的环境变量就越多。</p>\n<p>我们的 <code>bayesjob</code> crd 里恰好有几个 service 那么所有启动的 pod 里就会默认多非常多的环境变量，然后恰好那天有个用户跑了非常多的 <code>bayesjob</code> 刚好冲破了这个程序启动注入环境变量的上限就引发了这个问题。</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">SVC_XXXA_TXWGIWBFC37V_PORT_80_TCP_PORT=80\nSVC_BAADF_OE3QGDRQF7OX_PORT_6637_TCP_ADDR=10.97.0.126\nSVC_ADMIN_VI6RZ95HMNVY_PORT_7088_TCP_ADDR=10.97.8.254\nSVC_ADMIN_VI6RZ95HMNVY_SERVICE_PORT=80\nSVC_ADMIN_VI6RZ95HMNVY_PORT=tcp://10.97.8.254:80\n...</code></pre></div>\n<h2 id=\"解决方案\" style=\"position:relative;\">解决方案<a href=\"#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88\" aria-label=\"解决方案 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h2>\n<p>解决的办法就是在 pod 里增加额外的配置：<code>enableServiceLinks: false</code> 告诉 pod 不要主动去注入这些环境变量即可。</p>\n<h2 id=\"总结\" style=\"position:relative;\">总结<a href=\"#%E6%80%BB%E7%BB%93\" aria-label=\"总结 permalink\" class=\"custom-class after\"><svg aria-hidden=\"true\" height=\"20\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"20\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a></h2>\n<p>之前 <code>bayesjob</code> 的生命周期比较短，并没有遇到这个问题，直到最近调整了它的生命周期（从 72h -> 14d）带来大量存活的 service ，最后在这天又恰好来了一个创建 300+ 任务的用户。这种问题真的是要到一定规模你才能发现的，实现考虑到的难度有点大了。</p>\n<p>crd controller 也应该有更好的容错性，不能因为一台 master 出问题就跪了才对，后续要对其进行调整。</p>","tableOfContents":"<ul>\n<li><a href=\"#%E4%BA%8B%E6%95%85%E5%8E%9F%E5%9B%A0\">事故原因</a></li>\n<li><a href=\"#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88\">解决方案</a></li>\n<li><a href=\"#%E6%80%BB%E7%BB%93\">总结</a></li>\n</ul>","frontmatter":{"title":"k8s pod 无法启动，报错 argument list too long","date":"July 12, 2023","tags":["k8s","devops","问题复盘"]},"excerpt":"事故原因 前天晚上 k8s 集群出现了问题，一开始是有一台 master 的内存不够了，然后导致了…"}},"pageContext":{"id":"dd5ea23c-07d0-52ca-800e-752af7089f06"}},"staticQueryHashes":["26522286"],"slicesMap":{}}